{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ArjunDandagi/cs145/blob/main/cs_145_Hashing_Algos.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pybloom_live h3 faiss-cpu whoosh"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9Hs8jB86RrxC",
        "outputId": "a2ab31d9-117d-462b-f7f6-18271fd86582"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pybloom_live\n",
            "  Downloading pybloom_live-4.0.0.tar.gz (10 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting h3\n",
            "  Downloading h3-4.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Collecting faiss-cpu\n",
            "  Downloading faiss_cpu-1.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.4 kB)\n",
            "Collecting whoosh\n",
            "  Downloading Whoosh-2.7.4-py2.py3-none-any.whl.metadata (3.1 kB)\n",
            "Collecting bitarray>=0.3.4 (from pybloom_live)\n",
            "  Downloading bitarray-2.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (34 kB)\n",
            "Collecting xxhash>=3.0.0 (from pybloom_live)\n",
            "  Downloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.25.0 in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (1.26.4)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from faiss-cpu) (24.1)\n",
            "Downloading h3-4.1.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (993 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m993.6/993.6 kB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading faiss_cpu-1.9.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (27.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m27.5/27.5 MB\u001b[0m \u001b[31m13.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading Whoosh-2.7.4-py2.py3-none-any.whl (468 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m468.8/468.8 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitarray-2.9.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (288 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.4/288.4 kB\u001b[0m \u001b[31m6.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xxhash-3.5.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.1/194.1 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hBuilding wheels for collected packages: pybloom_live\n",
            "  Building wheel for pybloom_live (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pybloom_live: filename=pybloom_live-4.0.0-py3-none-any.whl size=9228 sha256=e13b129a7cd7bd35525e0658658dd6b871eff340c2f6378bb6703c52077fa746\n",
            "  Stored in directory: /root/.cache/pip/wheels/36/8a/9f/83ab00e9a9b2b10cec6135fa2a6cd92a22edf3d87fdaae481e\n",
            "Successfully built pybloom_live\n",
            "Installing collected packages: whoosh, bitarray, xxhash, h3, faiss-cpu, pybloom_live\n",
            "Successfully installed bitarray-2.9.3 faiss-cpu-1.9.0 h3-4.1.1 pybloom_live-4.0.0 whoosh-2.7.4 xxhash-3.5.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Basic hash function\n",
        "import hashlib\n",
        "def hash_value(value, r=8):\n",
        "  if isinstance(value, int):\n",
        "    return hash(value) % r\n",
        "  sha256 = hashlib.sha256()\n",
        "  sha256.update(value.encode('utf-8'))\n",
        "  return int(sha256.hexdigest(), 16)"
      ],
      "metadata": {
        "id": "27jDrRYf09DP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'{hash_value(\"String 42 is making me hungry\")=}')\n",
        "print(f'{hash_value(\"String 43 is making me hungry\")=}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GbolvyVYurkK",
        "outputId": "0b36e9f2-9c52-4315-9960-933b7ae078cd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hash_value(\"String 42 is making me hungry\")=115370750186445326290377683743957786138830981293626300636188810120500006517549\n",
            "hash_value(\"String 43 is making me hungry\")=43094278518137645842364210990503851658090982228390781508461084004583964858452\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f'{hash_value(421, 8)=}')\n",
        "print(f'{hash_value(1523789, 8)=}')\n",
        "print(f'{hash_value(\"Crazy\")=}')\n",
        "print(f'{hash_value(\"Crazy\")=}')\n",
        "print(f'{hash_value(\"Taylor Swift\")=}')\n",
        "print(f'{hash_value(\"(37.428226, -122.174722)\")=}') # Stanford NVidia\n",
        "\n",
        "print(f'{hash_value(\"String 42 is making me hungry\")=}')\n",
        "print(f'{hash_value(\"String 43 is making me hungry\")=}')\n",
        "print(f'{hash_value(\"(37.428226, -122.174722)\")=}') # Stanford NVidia\n",
        "print(f'{hash_value(\"(37.429749, -122.173549)\")=}') # Stanford Gates\n",
        "print(f'{hash_value(\"(37.428226, -122.174722)\")=}') # Stanford NVidia\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AcqGez5t1cBY",
        "outputId": "bb3bdbae-d019-450a-e97d-cc84133e947f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hash_value(421, 8)=5\n",
            "hash_value(1523789, 8)=5\n",
            "hash_value(\"Crazy\")=26532752625765826853685626387507575286736774457388345789099730774055940208863\n",
            "hash_value(\"Crazy\")=26532752625765826853685626387507575286736774457388345789099730774055940208863\n",
            "hash_value(\"Taylor Swift\")=88112730712611275922716045032483444911241879675316572061569441555880719989902\n",
            "hash_value(\"(37.428226, -122.174722)\")=15584662638934651596674661938880452430485871690911786775291271296878253906639\n",
            "hash_value(\"String 42 is making me hungry\")=115370750186445326290377683743957786138830981293626300636188810120500006517549\n",
            "hash_value(\"String 43 is making me hungry\")=43094278518137645842364210990503851658090982228390781508461084004583964858452\n",
            "hash_value(\"(37.428226, -122.174722)\")=15584662638934651596674661938880452430485871690911786775291271296878253906639\n",
            "hash_value(\"(37.429749, -122.173549)\")=96461388757156338915254904484651934017607226452294372348554236985010151672039\n",
            "hash_value(\"(37.428226, -122.174722)\")=15584662638934651596674661938880452430485871690911786775291271296878253906639\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from whoosh.fields import TEXT, Schema\n",
        "from whoosh.index import create_in\n",
        "from whoosh.qparser import QueryParser\n",
        "\n",
        "# Define the index schema\n",
        "schema = Schema(content=TEXT)\n",
        "os.mkdir(\"my_index_dir2\")\n",
        "# Create a new index\n",
        "ix = create_in(\"my_index_dir2\", schema)\n",
        "\n",
        "# Add some documents to the index\n",
        "with ix.writer() as writer:\n",
        "  writer.add_document(content=\"crazy in love - beyonce\")\n",
        "  writer.add_document(content=\"crazy love - van morrison\")\n",
        "  writer.add_document(content=\"crazy -- aerosmith\")\n",
        "\n",
        "# Search for documents containing the word \"fox\"\n",
        "qries = [\"crazy\", \"aerosmith\", \"crazy love\"]\n",
        "for q in qries:\n",
        "  with ix.searcher() as searcher:\n",
        "    results = searcher.search(QueryParser(\"content\", schema).parse(q))\n",
        "    print(results)\n"
      ],
      "metadata": {
        "id": "iWjJi7l0dLN_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64e9d923-f552-4e48-bf17-0305ff89f09c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<Top 3 Results for Term('content', 'crazy') runtime=0.0004599319999982754>\n",
            "<Top 1 Results for Term('content', 'aerosmith') runtime=0.00039411400000233243>\n",
            "<Top 2 Results for And([Term('content', 'crazy'), Term('content', 'love')]) runtime=0.0008488379999960216>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "l4CeS-WmP0Ui",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        },
        "outputId": "fde89335-67b8-4093-9732-da45acabb00c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "AttributeError",
          "evalue": "module 'h3' has no attribute 'geo_to_h3'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-1b91b221bb6e>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mzoom\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m12\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;31m# zoomlevels 8 and 12 (imagine the +/- in map zoom)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mlocations\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mcell_id\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh3\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeo_to_h3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzoom\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"@zoom[\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mzoom\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"]:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mAttributeError\u001b[0m: module 'h3' has no attribute 'geo_to_h3'"
          ]
        }
      ],
      "source": [
        "import h3\n",
        "# lat/lngs from SF mission, Stanford NVidia, Packard, Gates Buildings\n",
        "locations = [(37.788022, -122.399797), (37.428226, -122.174722),\n",
        "             (37.429749, -122.1735490), (37.429761, -122.173290)]\n",
        "for zoom in [8, 12]: # zoomlevels 8 and 12 (imagine the +/- in map zoom)\n",
        "  for l in locations:\n",
        "    cell_id = h3.geo_to_h3(l[0], l[1], zoom)\n",
        "    print(\"@zoom[\", zoom, \"]:\", cell_id)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pybloom_live import BloomFilter\n",
        "\n",
        "# Create a Bloom filter with 1000 items and a 0.1% false positive rate\n",
        "bf = BloomFilter(capacity=1000, error_rate=0.001)\n",
        "\n",
        "# Add some integers to the Bloom filter\n",
        "bf.add(42); bf.add(30); bf.add(50)\n",
        "\n",
        "# Check if each integer in the list is in the Bloom filter\n",
        "integer_list = [42, 50, 100, 32]\n",
        "for i in integer_list:\n",
        "  if i in bf:\n",
        "    print(f\"{i} may be in the Bloom filter\")\n",
        "  else:\n",
        "    print(f\"{i} is definitely not in the Bloom filter\")"
      ],
      "metadata": {
        "id": "OHG_WGX3ayYa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np, faiss\n",
        "# Generate 1000 points in a 5000-dimensional space\n",
        "X = np.random.rand(10000, 5000).astype('float32')\n",
        "\n",
        "# Build an LSH index for the points\n",
        "index = faiss.IndexLSH(5000, 16)\n",
        "index.add(X)\n",
        "\n",
        "# Find the 5 nearest neighbors (distances and points) of xq, a new 10-dim point\n",
        "xq = np.random.rand(5000).astype('float32')\n",
        "D, I = index.search(np.expand_dims(xq, axis=0), k=5)\n",
        "\n",
        "# Print the indices and distances of the nearest neighbors\n",
        "print(\"Nearest neighbors of \", xq, \" are:\", I, \" with distance: \", D)"
      ],
      "metadata": {
        "id": "LYhsf-tsa2iz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sentence-transformers\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "\n",
        "model = SentenceTransformer('paraphrase-distilroberta-base-v1')\n",
        "\n",
        "text = \"String 42 is making me thirsty.\"\n",
        "embedding = model.encode(text)\n",
        "\n",
        "print(embedding)"
      ],
      "metadata": {
        "id": "a_sVcV53UcRQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "\n",
        "# Create 1000 example strings\n",
        "strings = [f\"String {i} is making me hungry\" for i in range(1000)]\n",
        "\n",
        "# Load the SentenceTransformer model\n",
        "model = SentenceTransformer('paraphrase-distilroberta-base-v1')\n",
        "\n",
        "# Compute embeddings for the strings\n",
        "embeddings = model.encode(strings)\n",
        "\n",
        "# Create a FAISS index\n",
        "index = faiss.IndexFlatL2(embeddings.shape[1])\n",
        "\n",
        "# Add the embeddings to the index\n",
        "index.add(np.array(embeddings))\n",
        "\n",
        "# Define a query string and compute its embedding\n",
        "for query_string in [\"String 42 is making me hungry\",\n",
        "                     \"String 42\", \"String 42 is making me thirsty\"]:\n",
        "  query_embedding = model.encode(query_string)\n",
        "\n",
        "  # Search the index for the nearest neighbors of the query embedding\n",
        "  k = 5  # Number of nearest neighbors to retrieve\n",
        "  distances, indices = index.search(np.array([query_embedding]), k)\n",
        "\n",
        "  print(f'{query_string=} ==> ')\n",
        "  # Print the results\n",
        "  for i, idx in enumerate(indices[0]):\n",
        "    print(f\"Rank {i+1}: {strings[idx]}, Distance: {distances[0][i]}\")\n"
      ],
      "metadata": {
        "id": "lP87dM3kVRM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "\n",
        "# Create a sample dataframe\n",
        "df = pd.DataFrame({\n",
        "    'col1': [1, 2, 3, 4],\n",
        "    'col2': ['foo', 'bar', 'baz', 'qux'],\n",
        "    'col3': [0.1, 0.2, 0.3, 0.4],\n",
        "})\n",
        "\n",
        "# Convert the dataframe to an Arrow table\n",
        "table = pa.Table.from_pandas(df)\n",
        "\n",
        "# Write the table to a Parquet file\n",
        "pq.write_table(table, 'example.parquet', compression='snappy')\n",
        "\n",
        "# Read the Parquet file back into an Arrow table\n",
        "table_from_file = pq.read_table('example.parquet')\n",
        "\n",
        "# Convert the Arrow table back to a Pandas dataframe\n",
        "df_from_file = table_from_file.to_pandas()\n",
        "\n",
        "# Print the original and read dataframes to verify they match\n",
        "print(df)\n",
        "print(df_from_file)"
      ],
      "metadata": {
        "id": "9axJANgyKJK9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}